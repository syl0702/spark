{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "454d264b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/20 13:20:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://0.0.0.0:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>taxi-fare-predict</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f829c4c0b20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"taxi-fare-predict\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10fb351f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:===================================================>     (10 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "directory = \"/home/ubuntu/working/datasource\"\n",
    "trip_files = \"/trips/*\"\n",
    "\n",
    "trips_df = spark.read.csv(f\"file:///{directory}/{trip_files}\", inferSchema=True, header=True)\n",
    "trips_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac505779",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df.createOrReplaceTempView(\"trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6051a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 정제\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    t.passenger_count,\n",
    "    PULocationID as pickup_location_id,\n",
    "    DOLocationID as dropoff_location_id,\n",
    "    t.trip_distance,\n",
    "    HOUR(tpep_pickup_datetime) as pickup_time,\n",
    "    DATE_FORMAT(TO_DATE(tpep_pickup_datetime), 'EEEE') as day_of_week,\n",
    "    \n",
    "    t.total_amount\n",
    "\n",
    "FROM trips t\n",
    "\n",
    "WHERE t.total_amount < 200\n",
    "  AND t.total_amount > 0\n",
    "  AND t.passenger_count < 5\n",
    "  AND TO_DATE(t.tpep_pickup_datetime) >= '2021-01-01'\n",
    "  AND TO_DATE(t.tpep_pickup_datetime) < '2021-08-01'\n",
    "  AND t.trip_distance < 10\n",
    "  AND t.trip_distance > 0\n",
    "\"\"\"\n",
    "\n",
    "data_df = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad12b7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- pickup_location_id: integer (nullable = true)\n",
      " |-- dropoff_location_id: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_time: integer (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5a2ab4",
   "metadata": {},
   "source": [
    "Train / Test 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b5a83d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sdf, test_sdf = data_df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2b55da",
   "metadata": {},
   "source": [
    "만약에 데이터의 양이 너무 많고, 그 데이터를 오랜 시간 들여서 전처리를 다 완료 했다고 가정.\n",
    "- 여러 모델을 만들거나 실험을 할 때에도 위의 전처리 작업을 그대로 매번 수행\n",
    "- 추후에 다시 이 데이터를 활용한다면 시간이 많이 걸릴 듯.\n",
    "- 처리가 완료된 데이터를 파일이나 데이터베이스에 저장해 놓고 나중에 불러오는게 더 빠르다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c295008c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/home/ubuntu/working/spark-examples/data/ml-data/train already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m save_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/ubuntu/working/spark-examples/data/ml-data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Spark DataFrame의 write 메소드를 이용해 데이터를 파일 또는 데이터베이스에 저장할 수 있다.\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrain_sdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msave_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/train/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m test_sdf\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/test/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/spark-env/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/miniconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path file:/home/ubuntu/working/spark-examples/data/ml-data/train already exists."
     ]
    }
   ],
   "source": [
    "# 파케이 (parquet) 형식으로 저장\n",
    "save_dir = \"/home/ubuntu/working/spark-examples/data/ml-data\"\n",
    "\n",
    "# Spark DataFrame의 write 메소드를 이용해 데이터를 파일 또는 데이터베이스에 저장할 수 있다.\n",
    "train_sdf.write.format(\"parquet\").save(f\"{save_dir}/train/\")\n",
    "test_sdf.write.format(\"parquet\").save(f\"{save_dir}/test/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac09266",
   "metadata": {},
   "source": [
    "## 파이프라인 정의\n",
    "파이프라인 정의를 위한 stage 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c189e13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인에 넣을 과정을 모아 놓을 리스트\n",
    "stages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c4f300",
   "metadata": {},
   "source": [
    "## OneHotEncoding Stage\n",
    "- `pickup_location_id`\n",
    "- `dropoff_location_id`\n",
    "- `day_of_week`\n",
    "`pickup_location_id`, `dropoff_location_id`는 숫자 형식의 데이터\n",
    "- 숫자 형식의 데이터는 `OneHotEncoding` 불가능\n",
    "- `StringIndexer` Transformer를 활용해 숫자형 데이터를 문자열로 취급하게끔 할 수 있다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4c61848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_b3bdf7bf15b9,\n",
       " OneHotEncoder_ea0d7852607c,\n",
       " StringIndexer_c014ecc6fa86,\n",
       " OneHotEncoder_3ddbfd6a021f,\n",
       " StringIndexer_915d8dd92be7,\n",
       " OneHotEncoder_57945aa853f5]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# OneHotEncoding을 수행할 컬럼\n",
    "cat_features = [\n",
    "    \"pickup_location_id\",\n",
    "    \"dropoff_location_id\",\n",
    "    \"day_of_week\"\n",
    "]\n",
    "\n",
    "for c in cat_features:\n",
    "    # 1. 데이터를 문자열 형식으로 바꿔준다.\n",
    "    cat_indexer = StringIndexer(inputCol=c, outputCol=c+\"_idx\").setHandleInvalid(\"keep\")\n",
    "    \n",
    "    # 2. OneHotEnoding 수행\n",
    "    onehot_encoder = OneHotEncoder(\n",
    "        inputCols=[cat_indexer.getOutputCol()],\n",
    "        outputCols=[c+\"_onehot\"]\n",
    "    )\n",
    "    \n",
    "    stages += [cat_indexer, onehot_encoder]\n",
    "    \n",
    "stages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e420929",
   "metadata": {},
   "source": [
    "## Standard Scaling Stage\n",
    "- 숫자형 데이터들에 대한 표준화 수행\n",
    "- `passenger_count`, `trip_distance`, `pickup_time`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6461abc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_b3bdf7bf15b9,\n",
       " OneHotEncoder_ea0d7852607c,\n",
       " StringIndexer_c014ecc6fa86,\n",
       " OneHotEncoder_3ddbfd6a021f,\n",
       " StringIndexer_915d8dd92be7,\n",
       " OneHotEncoder_57945aa853f5,\n",
       " VectorAssembler_313751fa088c,\n",
       " StandardScaler_07d14930e4ff,\n",
       " VectorAssembler_bf68b41381be,\n",
       " StandardScaler_8c883075c871,\n",
       " VectorAssembler_e095120a623f,\n",
       " StandardScaler_191a9ed7ae19]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 컬럼의 데이터를 벡터화 시키고, Standard Scaling을 수행\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "num_features = [\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"pickup_time\"\n",
    "]\n",
    "\n",
    "for n in num_features:\n",
    "    # 1. 벡터화 스테이지\n",
    "    num_assembler = VectorAssembler(inputCols=[n], outputCol=n+\"_vector\")\n",
    "    \n",
    "    # 2. StandardScaler 스테이지\n",
    "    num_scaler = StandardScaler(inputCol=num_assembler.getOutputCol(), outputCol=n+\"_scaled\")\n",
    "    \n",
    "    stages += [num_assembler, num_scaler]\n",
    "    \n",
    "stages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a96ed01",
   "metadata": {},
   "source": [
    "## Feature Assemble Stage\n",
    "- 컬럼 명 뒤에 `_onehot`이 붙거나 `_scaled`가 붙은 컬럼만 Feature Vector로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afa4eb09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pickup_location_id_onehot',\n",
       " 'dropoff_location_id_onehot',\n",
       " 'day_of_week_onehot',\n",
       " 'passenger_count_scaled',\n",
       " 'trip_distance_scaled',\n",
       " 'pickup_time_scaled']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assembler_inputs = [ c +\"_onehot\" for c in cat_features ] + [ n + \"_scaled\" for n in num_features]\n",
    "assembler_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09cf1d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_b3bdf7bf15b9,\n",
       " OneHotEncoder_ea0d7852607c,\n",
       " StringIndexer_c014ecc6fa86,\n",
       " OneHotEncoder_3ddbfd6a021f,\n",
       " StringIndexer_915d8dd92be7,\n",
       " OneHotEncoder_57945aa853f5,\n",
       " VectorAssembler_313751fa088c,\n",
       " StandardScaler_07d14930e4ff,\n",
       " VectorAssembler_bf68b41381be,\n",
       " StandardScaler_8c883075c871,\n",
       " VectorAssembler_e095120a623f,\n",
       " StandardScaler_191a9ed7ae19,\n",
       " VectorAssembler_c77edbb511e9]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "\n",
    "stages.append(feature_assembler)\n",
    "stages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c474c40",
   "metadata": {},
   "source": [
    "## Pipeline 구성\n",
    "순서대로 구성된 stage를 한꺼번에 수행할 파이프라인 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23747e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "transform_stages = stages\n",
    "pipeline = Pipeline(stages=transform_stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36d7ced",
   "metadata": {},
   "source": [
    "## 데이터를 파이프라인에 통과시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee1cd605",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function JavaWrapper.__del__ at 0x7f8261413940>          \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/ml/wrapper.py\", line 39, in __del__\n",
      "    if SparkContext._active_spark_context and self._java_obj is not None:\n",
      "AttributeError: 'VectorAssembler' object has no attribute '_java_obj'\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# transformer의 fit : 변환을 하기 위한 수 또는 방법을 구하는 과정\n",
    "fitted_transformer = pipeline.fit(train_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6899fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- pickup_location_id: integer (nullable = true)\n",
      " |-- dropoff_location_id: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_time: integer (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- pickup_location_id_idx: double (nullable = false)\n",
      " |-- pickup_location_id_onehot: vector (nullable = true)\n",
      " |-- dropoff_location_id_idx: double (nullable = false)\n",
      " |-- dropoff_location_id_onehot: vector (nullable = true)\n",
      " |-- day_of_week_idx: double (nullable = false)\n",
      " |-- day_of_week_onehot: vector (nullable = true)\n",
      " |-- passenger_count_vector: vector (nullable = true)\n",
      " |-- passenger_count_scaled: vector (nullable = true)\n",
      " |-- trip_distance_vector: vector (nullable = true)\n",
      " |-- trip_distance_scaled: vector (nullable = true)\n",
      " |-- pickup_time_vector: vector (nullable = true)\n",
      " |-- pickup_time_scaled: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transform : 데이터를 변환\n",
    "vec_train_sdf = fitted_transformer.transform(train_sdf)\n",
    "vec_train_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d05e9a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 20:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|            features|total_amount|\n",
      "+--------------------+------------+\n",
      "|(532,[62,311,528,...|         6.8|\n",
      "|(532,[62,311,526,...|        15.3|\n",
      "|(532,[62,280,523,...|         8.3|\n",
      "|(532,[62,308,522,...|        15.8|\n",
      "|(532,[62,291,524,...|        19.2|\n",
      "+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "vec_train_sdf.select(\"features\", \"total_amount\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839c2986",
   "metadata": {},
   "source": [
    "## 모델 생성 및 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e1c12f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(\n",
    "    maxIter = 50,\n",
    "    solver = 'normal', # 최적화 방식\n",
    "    labelCol = 'total_amount',\n",
    "    featuresCol = 'features'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6222af9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/20 14:25:17 WARN Instrumentation: [dd10f711] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/11/20 14:25:47 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/11/20 14:25:47 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "23/11/20 14:27:40 WARN InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "23/11/20 14:27:40 WARN Instrumentation: [dd10f711] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "23/11/20 14:27:41 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/11/20 14:27:41 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lr_model = lr.fit(vec_train_sdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb403ea0",
   "metadata": {},
   "source": [
    "## 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c88d2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인을 이용해 test_sdf 변환\n",
    "vec_test_sdf = fitted_transformer.transform(test_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef00a322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vec_test_sdf로 예측\n",
    "predictions = lr_model.transform(vec_test_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e947f451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 25:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+-------------------+-------------+-----------+-----------+------------+----------------------+-------------------------+-----------------------+--------------------------+---------------+------------------+----------------------+----------------------+--------------------+--------------------+------------------+--------------------+--------------------+------------------+\n",
      "|passenger_count|pickup_location_id|dropoff_location_id|trip_distance|pickup_time|day_of_week|total_amount|pickup_location_id_idx|pickup_location_id_onehot|dropoff_location_id_idx|dropoff_location_id_onehot|day_of_week_idx|day_of_week_onehot|passenger_count_vector|passenger_count_scaled|trip_distance_vector|trip_distance_scaled|pickup_time_vector|  pickup_time_scaled|            features|        prediction|\n",
      "+---------------+------------------+-------------------+-------------+-----------+-----------+------------+----------------------+-------------------------+-----------------------+--------------------------+---------------+------------------+----------------------+----------------------+--------------------+--------------------+------------------+--------------------+--------------------+------------------+\n",
      "|              0|                 4|                 48|          2.8|         16|   Saturday|        19.3|                  62.0|         (262,[62],[1.0])|                   12.0|          (260,[12],[1.0])|            4.0|     (7,[4],[1.0])|                 [0.0]|                 [0.0]|               [2.8]|[1.5840134456481147]|            [16.0]| [3.155517043172724]|(532,[62,274,526,...|17.906918373303213|\n",
      "|              0|                 4|                107|          1.0|         12|   Thursday|       12.25|                  62.0|         (262,[62],[1.0])|                   17.0|          (260,[17],[1.0])|            1.0|     (7,[1],[1.0])|                 [0.0]|                 [0.0]|               [1.0]|[0.5657190877314696]|            [12.0]|[2.3666377823795433]|(532,[62,279,523,...|11.758641785745093|\n",
      "|              0|                 4|                114|          0.9|          9|   Thursday|       11.15|                  62.0|         (262,[62],[1.0])|                   36.0|          (260,[36],[1.0])|            1.0|     (7,[1],[1.0])|                 [0.0]|                 [0.0]|               [0.9]|[0.5091471789583226]|             [9.0]|[1.7749783367846574]|(532,[62,298,523,...|11.687663923241496|\n",
      "|              0|                 4|                148|          0.8|         20|   Saturday|         9.8|                  62.0|         (262,[62],[1.0])|                   39.0|          (260,[39],[1.0])|            4.0|     (7,[4],[1.0])|                 [0.0]|                 [0.0]|               [0.8]|[0.45257527018517...|            [20.0]| [3.944396303965905]|(532,[62,301,526,...|11.308533187011903|\n",
      "|              0|                 4|                209|          2.6|         14|    Tuesday|       15.41|                  62.0|         (262,[62],[1.0])|                   60.0|          (260,[60],[1.0])|            3.0|     (7,[3],[1.0])|                 [0.0]|                 [0.0]|               [2.6]| [1.470869628101821]|            [14.0]|[2.7610774127761335]|(532,[62,322,525,...| 16.97472258970197|\n",
      "|              0|                 4|                234|          1.5|         14|    Tuesday|        12.8|                  62.0|         (262,[62],[1.0])|                   11.0|          (260,[11],[1.0])|            3.0|     (7,[3],[1.0])|                 [0.0]|                 [0.0]|               [1.5]|[0.8485786315972044]|            [14.0]|[2.7610774127761335]|(532,[62,273,525,...|13.548759324683143|\n",
      "|              0|                 7|                 70|          4.1|         16|  Wednesday|        16.3|                  63.0|         (262,[63],[1.0])|                  107.0|         (260,[107],[1.0])|            2.0|     (7,[2],[1.0])|                 [0.0]|                 [0.0]|               [4.1]|[2.3194482596990253]|            [16.0]| [3.155517043172724]|(532,[63,369,524,...|19.121522380743787|\n",
      "|              0|                 7|                145|          1.8|         13|     Sunday|         8.8|                  63.0|         (262,[63],[1.0])|                   58.0|          (260,[58],[1.0])|            6.0|     (7,[6],[1.0])|                 [0.0]|                 [0.0]|               [1.8]|[1.0182943579166452]|            [13.0]|[2.5638575975778384]|(532,[63,320,528,...| 11.86784157404618|\n",
      "|              0|                 7|                238|          6.0|          6|     Friday|        25.0|                  63.0|         (262,[63],[1.0])|                    7.0|           (260,[7],[1.0])|            0.0|     (7,[0],[1.0])|                 [0.0]|                 [0.0]|               [6.0]|[3.3943145263888175]|             [6.0]|[1.1833188911897716]|(532,[63,269,522,...|28.049947107583073|\n",
      "|              0|                 7|                260|          3.1|          3|   Saturday|        12.3|                  63.0|         (262,[63],[1.0])|                   82.0|          (260,[82],[1.0])|            4.0|     (7,[4],[1.0])|                 [0.0]|                 [0.0]|               [3.1]|[1.7537291719675558]|             [3.0]|[0.5916594455948858]|(532,[63,344,526,...|13.899130630614604|\n",
      "|              0|                10|                 93|          6.2|         15|     Monday|        0.31|                 118.0|        (262,[118],[1.0])|                  163.0|         (260,[163],[1.0])|            5.0|     (7,[5],[1.0])|                 [0.0]|                 [0.0]|               [6.2]|[3.5074583439351117]|            [15.0]| [2.958297227974429]|(532,[118,425,527...|29.192941653695726|\n",
      "|              0|                10|                132|          4.6|          2|     Friday|        17.8|                 118.0|        (262,[118],[1.0])|                   85.0|          (260,[85],[1.0])|            0.0|     (7,[0],[1.0])|                 [0.0]|                 [0.0]|               [4.6]|  [2.60230780356476]|             [2.0]|[0.3944396303965905]|(532,[118,347,522...| 38.75091497037384|\n",
      "|              0|                12|                 45|          2.5|         21|   Saturday|       17.75|                  71.0|         (262,[71],[1.0])|                   59.0|          (260,[59],[1.0])|            4.0|     (7,[4],[1.0])|                 [0.0]|                 [0.0]|               [2.5]| [1.414297719328674]|            [21.0]|   [4.1416161191642]|(532,[71,321,526,...|15.900680118656872|\n",
      "|              0|                12|                100|          6.3|         17|     Monday|        24.3|                  71.0|         (262,[71],[1.0])|                   30.0|          (260,[30],[1.0])|            5.0|     (7,[5],[1.0])|                 [0.0]|                 [0.0]|               [6.3]|[3.5640302527082586]|            [17.0]|[3.3527368583710193]|(532,[71,292,527,...|30.486533958538615|\n",
      "|              0|                12|                233|          5.2|         13|     Monday|       23.15|                  71.0|         (262,[71],[1.0])|                   27.0|          (260,[27],[1.0])|            5.0|     (7,[5],[1.0])|                 [0.0]|                 [0.0]|               [5.2]| [2.941739256203642]|            [13.0]|[2.5638575975778384]|(532,[71,289,527,...|25.748931820423426|\n",
      "|              0|                13|                 13|          0.5|         15|     Friday|        9.45|                  47.0|         (262,[47],[1.0])|                   44.0|          (260,[44],[1.0])|            0.0|     (7,[0],[1.0])|                 [0.0]|                 [0.0]|               [0.5]|[0.2828595438657348]|            [15.0]| [2.958297227974429]|(532,[47,306,522,...| 8.719454546770034|\n",
      "|              0|                13|                 45|          2.0|         16|     Monday|        15.8|                  47.0|         (262,[47],[1.0])|                   59.0|          (260,[59],[1.0])|            5.0|     (7,[5],[1.0])|                 [0.0]|                 [0.0]|               [2.0]|[1.1314381754629392]|            [16.0]| [3.155517043172724]|(532,[47,321,527,...|14.298919880066531|\n",
      "|              0|                13|                 88|          0.9|         19|   Thursday|         9.8|                  47.0|         (262,[47],[1.0])|                   56.0|          (260,[56],[1.0])|            1.0|     (7,[1],[1.0])|                 [0.0]|                 [0.0]|               [0.9]|[0.5091471789583226]|            [19.0]|  [3.74717648876761]|(532,[47,318,523,...| 9.233141687921048|\n",
      "|              0|                13|                107|          5.1|         17|     Friday|        21.8|                  47.0|         (262,[47],[1.0])|                   17.0|          (260,[17],[1.0])|            0.0|     (7,[0],[1.0])|                 [0.0]|                 [0.0]|               [5.1]|[2.8851673474304946]|            [17.0]|[3.3527368583710193]|(532,[47,279,522,...| 26.42642567582282|\n",
      "|              0|                13|                113|          2.2|         10|  Wednesday|        14.3|                  47.0|         (262,[47],[1.0])|                   32.0|          (260,[32],[1.0])|            2.0|     (7,[2],[1.0])|                 [0.0]|                 [0.0]|               [2.2]|[1.2445819930092332]|            [10.0]|[1.9721981519829526]|(532,[47,294,524,...| 15.52606046164573|\n",
      "+---------------+------------------+-------------------+-------------+-----------+-----------+------------+----------------------+-------------------------+-----------------------+--------------------------+---------------+------------------+----------------------+----------------------+--------------------+--------------------+------------------+--------------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a933068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[passenger_count: int, pickup_location_id: int, dropoff_location_id: int, trip_distance: double, pickup_time: int, day_of_week: string, total_amount: double, pickup_location_id_idx: double, pickup_location_id_onehot: vector, dropoff_location_id_idx: double, dropoff_location_id_onehot: vector, day_of_week_idx: double, day_of_week_onehot: vector, passenger_count_vector: vector, passenger_count_scaled: vector, trip_distance_vector: vector, trip_distance_scaled: vector, pickup_time_vector: vector, pickup_time_scaled: vector, features: vector, prediction: double]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측한 결과를 따로 확인할 때는 일반적으로 조회만 일어나기 때문에 캐시 처리는 해 주는 것이 좋다.\n",
    "predictions.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9557fba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 26:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+------------+------------------+\n",
      "|trip_distance|day_of_week|total_amount|        prediction|\n",
      "+-------------+-----------+------------+------------------+\n",
      "|          2.8|   Saturday|        19.3|17.906918373303213|\n",
      "|          1.0|   Thursday|       12.25|11.758641785745093|\n",
      "|          0.9|   Thursday|       11.15|11.687663923241496|\n",
      "|          0.8|   Saturday|         9.8|11.308533187011903|\n",
      "|          2.6|    Tuesday|       15.41| 16.97472258970197|\n",
      "|          1.5|    Tuesday|        12.8|13.548759324683143|\n",
      "|          4.1|  Wednesday|        16.3|19.121522380743787|\n",
      "|          1.8|     Sunday|         8.8| 11.86784157404618|\n",
      "|          6.0|     Friday|        25.0|28.049947107583073|\n",
      "|          3.1|   Saturday|        12.3|13.899130630614604|\n",
      "|          6.2|     Monday|        0.31|29.192941653695726|\n",
      "|          4.6|     Friday|        17.8| 38.75091497037384|\n",
      "|          2.5|   Saturday|       17.75|15.900680118656872|\n",
      "|          6.3|     Monday|        24.3|30.486533958538615|\n",
      "|          5.2|     Monday|       23.15|25.748931820423426|\n",
      "|          0.5|     Friday|        9.45| 8.719454546770034|\n",
      "|          2.0|     Monday|        15.8|14.298919880066531|\n",
      "|          0.9|   Thursday|         9.8| 9.233141687921048|\n",
      "|          5.1|     Friday|        21.8| 26.42642567582282|\n",
      "|          2.2|  Wednesday|        14.3| 15.52606046164573|\n",
      "+-------------+-----------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.select(\"trip_distance\", \"day_of_week\", \"total_amount\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5dd537a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2664868984555686"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29293280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7947353376551619"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d99a679",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e579e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5fc418",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53961ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
